{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.080286Z",
     "start_time": "2022-02-02T08:27:26.745474Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.095382Z",
     "start_time": "2022-02-02T08:27:28.085104Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.113696Z",
     "start_time": "2022-02-02T08:27:28.100249Z"
    }
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab.pad_id)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d_model)\n",
    "        self.seg_emb = nn.Embedding(3, d_model, padding_idx=vocab.pad_id)\n",
    "\n",
    "    def generate_enc_mask_m(self, src) :       \n",
    "        mask_m = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        return mask_m\n",
    "    \n",
    "    def forward(self, txt, seg) : \n",
    "        emb = self.tok_emb(txt)\n",
    "        pos = torch.arange(0, emb.shape[1]).unsqueeze(0).repeat(emb.shape[0], 1).to(emb.device)\n",
    "        summed = emb + self.pos_emb(pos) + self.seg_emb(seg)\n",
    "        return summed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.135210Z",
     "start_time": "2022-02-02T08:27:28.117721Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module) : \n",
    "    def __init__(self, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask) :         \n",
    "        score = torch.matmul(q, k.permute(0,1,3,2).contiguous())/math.sqrt(d_model)\n",
    "        score = score.masked_fill(mask==0, -1e10)\n",
    "        scaled_score = torch.softmax(score, dim=-1)\n",
    "        \n",
    "        attention = torch.matmul(scaled_score, v).permute(0,2,3,1).contiguous()\n",
    "        attention = attention.view(attention.shape[0], attention.shape[1], self.d_model)\n",
    "        \n",
    "        return self.fc(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.151155Z",
     "start_time": "2022-02-02T08:27:28.139046Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) : \n",
    "    def __init__(self, d_model, seq_length, n_head) : \n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, f\"n_head({n_head}) does not divide d_model({d_model})\"\n",
    "\n",
    "        self.n_div_head = d_model//n_head\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_length\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.Q = nn.Linear(d_model,  d_model)\n",
    "        self.K = nn.Linear(d_model,  d_model)\n",
    "        self.V = nn.Linear(d_model,  d_model)\n",
    "        \n",
    "    def div_and_sort_for_multiheads(self, projected, seq_len) : \n",
    "        div = projected.view(projected.shape[0], self.n_head, seq_len, self.n_div_head)\n",
    "        return div\n",
    "    \n",
    "    def forward(self, emb) :\n",
    "        q = self.div_and_sort_for_multiheads(self.Q(emb), self.seq_len)    \n",
    "        k = self.div_and_sort_for_multiheads(self.K(emb), self.seq_len)\n",
    "        v = self.div_and_sort_for_multiheads(self.V(emb), self.seq_len)\n",
    "\n",
    "        return q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process the sub-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.160315Z",
     "start_time": "2022-02-02T08:27:28.154205Z"
    }
   },
   "outputs": [],
   "source": [
    "class PostProcessing(nn.Module) : \n",
    "    def __init__(self, d_model, p=0.1) : \n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, emb, attn) : \n",
    "        return emb+self.dropout(self.ln(attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.169204Z",
     "start_time": "2022-02-02T08:27:28.163085Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module) : \n",
    "    def __init__(self, d_model, d_ff) : \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        return self.fc2(self.act(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.182208Z",
     "start_time": "2022-02-02T08:27:28.173649Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model, d_ff, n_head, dropout_p) : \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ma = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.sdp = ScaledDotProductAttention(d_model)\n",
    "        \n",
    "        self.pp1 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp2 = PostProcessing(d_model, dropout_p)\n",
    "        \n",
    "        self.positionwise_ffn = PositionwiseFFN(d_model, d_ff)\n",
    "            \n",
    "    def forward(self, emb, mask_m) :\n",
    "\n",
    "        q,k,v = self.ma(emb)    \n",
    "        attn = self.sdp(q,k,v, mask=mask_m)\n",
    "        \n",
    "        attn = self.pp1(emb, attn)\n",
    "        z = self.positionwise_ffn(attn)\n",
    "\n",
    "        return self.pp2(attn, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.192765Z",
     "start_time": "2022-02-02T08:27:28.185064Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module) : \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 seq_length,\n",
    "                 d_model,\n",
    "                 d_ff,\n",
    "                 n_head,\n",
    "                 dropout_p,\n",
    "                 n_enc_layer) : \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embber = InputEmbedding(vocab_size, seq_length, d_model)\n",
    "        \n",
    "        enc = EncoderLayer(vocab_size, seq_length, d_model, d_ff, n_head, dropout_p)\n",
    "        \n",
    "        self.enc = nn.ModuleList([deepcopy(enc) for _ in range(n_enc_layer)])\n",
    "\n",
    "    def forward(self, txt, seg) : \n",
    "        \n",
    "        emb = self.embber(txt, seg)\n",
    "        mask_m = self.embber.generate_enc_mask_m(txt)\n",
    "\n",
    "        for enc_layer in self.enc : \n",
    "            emb = enc_layer(emb, mask_m)\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.201718Z",
     "start_time": "2022-02-02T08:27:28.195413Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertFC(nn.Module) : \n",
    "    def __init__(self, embedder, d_model, vocab_size) : \n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.mlm_fc = nn.Linear(d_model, vocab_size)\n",
    "        self.nsp_fc = nn.Linear(d_model, 2)\n",
    "    \n",
    "    def forward(self, txt, seg) : \n",
    "        emb = self.embedder(txt, seg)\n",
    "        return torch.log_softmax(self.mlm_fc(emb), dim=-1), torch.log_softmax(self.nsp_fc(emb[:,0]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.494010Z",
     "start_time": "2022-02-02T08:27:28.204204Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "seq_len = 256\n",
    "\n",
    "train_dataset = iterator.BertIterator(filename = '../data/wikitext-2-raw/prep_train.txt',\n",
    "                                tokenizer_model_path = '../data/wiki_pretrained_vocab/m.model',\n",
    "                                seq_len=seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    " \n",
    "valid_dataset = iterator.BertIterator(filename = '../data/wikitext-2-raw/prep_valid.txt',\n",
    "                                tokenizer_model_path = '../data/wiki_pretrained_vocab/m.model',\n",
    "                                seq_len=seq_len)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:28.574549Z",
     "start_time": "2022-02-02T08:27:28.496511Z"
    }
   },
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "d_ff = d_model * 4\n",
    "n_head = 8\n",
    "vocab_size = 30000\n",
    "dropout_p = 0.1\n",
    "n_enc_layer = 1\n",
    "seq_length = train_dataset.seq_len\n",
    "vocab = train_dataset.vocab\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:32.822324Z",
     "start_time": "2022-02-02T08:27:28.578754Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_ember = BERT(vocab_size,\n",
    "             seq_length,\n",
    "             d_model,\n",
    "             d_ff,\n",
    "             n_head,\n",
    "             dropout_p,\n",
    "             n_enc_layer).to(device) \n",
    "# generate embeding using transformer architecture\n",
    "\n",
    "bert_predicter = nn.DataParallel(BertFC(bert_ember, d_model, vocab_size)).to(device)\n",
    "# return 2 fc layer for training bi-directional representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:32.831700Z",
     "start_time": "2022-02-02T08:27:32.825375Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True) # -- for debugging the train progress --\n",
    "N_EPOCHS = 20\n",
    "\n",
    "criterion1 = nn.NLLLoss(ignore_index=0).to(device)\n",
    "criterion2 = nn.NLLLoss().to(device)\n",
    "\n",
    "optimizer = optim.AdamW(bert_predicter.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-1, pct_start=0.01, \n",
    "                                          steps_per_epoch=len(train_dataloader), epochs=N_EPOCHS, \n",
    "                                          total_steps=N_EPOCHS * len(train_dataloader), anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T08:27:32.852436Z",
     "start_time": "2022-02-02T08:27:32.833437Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, scheduler):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_total_loss = 0\n",
    "    epoch_mlm_loss = 0\n",
    "    epoch_nsp_loss = 0 \n",
    "    \n",
    "    epoch_mlm_acc = 0    \n",
    "    epoch_nsp_acc = 0    \n",
    "    cnt = 0\n",
    "    \n",
    "    for data in tqdm(dataloader, desc='train') :\n",
    "        \n",
    "        data = {k:v.to(device) for k,v in data.items()}\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        mlm_pred, nsp_pred = model(data['text'], data['seg'])\n",
    "\n",
    "        # calculate loss from masked language modeling\n",
    "        mlm_loss = criterion1(mlm_pred.transpose(1,2), data['mlm'])\n",
    "        \n",
    "        # calculate loss from next sentence prediction\n",
    "        nsp_loss = criterion2(nsp_pred, data['nsp'].long().cuda())\n",
    "        \n",
    "        # merge two loss equally\n",
    "        loss = mlm_loss + nsp_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()        \n",
    "        \n",
    "        # calculate acc for mlm\n",
    "        acc = (mlm_pred.view(-1,vocab_size).argmax(1) == data['mlm'].view(-1)).sum().item() / data['mlm'].view(-1).shape[0]\n",
    "        epoch_mlm_acc += acc\n",
    "        \n",
    "        # calculate acc from nsp\n",
    "        acc = (nsp_pred.argmax(1) == data['nsp']).sum() / data['nsp'].shape[0]\n",
    "        epoch_nsp_acc += acc\n",
    "        \n",
    "        epoch_total_loss += loss.item()\n",
    "        epoch_mlm_loss += mlm_loss.item()\n",
    "        epoch_nsp_loss += nsp_loss.item()        \n",
    "        cnt += 1\n",
    "        scheduler.step()\n",
    "        \n",
    "    print(f'\\tTrain Total Loss: {epoch_total_loss / cnt:.3f} | Train MLM Loss: {epoch_mlm_loss / cnt:.3f} | Train NSP Loss: {epoch_nsp_loss / cnt:.3f}\\\n",
    "        | MLM ACC : {epoch_mlm_acc / cnt: .3f} | NSP ACC : {epoch_nsp_acc / cnt: .3f} | Learning Rate : {scheduler.get_last_lr()[0]:.3f}')\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_total_loss = 0\n",
    "    epoch_mlm_loss = 0\n",
    "    epoch_nsp_loss = 0 \n",
    "    \n",
    "    epoch_mlm_acc = 0    \n",
    "    epoch_nsp_acc = 0    \n",
    "    cnt = 0\n",
    "    \n",
    "    with torch.no_grad() : \n",
    "        for data in tqdm(dataloader, desc='valid') :\n",
    "            \n",
    "            data = {k:v.to(device) for k,v in data.items()}\n",
    "\n",
    "            mlm_pred, nsp_pred = model(data['text'], data['seg'])\n",
    "            \n",
    "            mlm_loss = criterion1(mlm_pred.transpose(1,2), data['mlm'])\n",
    "\n",
    "            nsp_loss = criterion2(nsp_pred, data['nsp'].long().cuda())\n",
    "            \n",
    "            loss = mlm_loss + nsp_loss\n",
    "            \n",
    "            acc = (mlm_pred.view(-1,vocab_size).argmax(1) == data['mlm'].view(-1)).sum().item() / data['mlm'].view(-1).shape[0]\n",
    "            epoch_mlm_acc += acc\n",
    "\n",
    "            acc = (nsp_pred.argmax(1) == data['nsp']).sum() / data['nsp'].shape[0]\n",
    "            epoch_nsp_acc += acc\n",
    "\n",
    "            epoch_total_loss += loss.item()\n",
    "            epoch_mlm_loss += mlm_loss.item()\n",
    "            epoch_nsp_loss += nsp_loss.item()        \n",
    "            cnt += 1\n",
    "\n",
    "        print(f'\\Valid Total Loss: {epoch_total_loss / cnt:.3f} | Valid MLM Loss: {epoch_mlm_loss / cnt:.3f} | Valid NSP Loss: {epoch_nsp_loss / cnt:.3f}\\\n",
    "            | MLM ACC : {epoch_mlm_acc / cnt: .3f} | NSP ACC : {epoch_nsp_acc / cnt: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-02T08:35:37.998Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a41d91da81c4c9db626f01b7b43ada3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):    \n",
    "    train(bert_predicter, train_dataloader, optimizer, scheduler)\n",
    "    evaluate(bert_predicter, train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
