{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.852553Z",
     "start_time": "2022-01-30T14:18:17.454916Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.860526Z",
     "start_time": "2022-01-30T14:18:18.855525Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.871237Z",
     "start_time": "2022-01-30T14:18:18.863267Z"
    }
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=train_dataset.prep.pad_id)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d_model)\n",
    "        self.seg_emb = nn.Embedding(3, d_model, padding_idx=train_dataset.prep.pad_id)\n",
    "        \n",
    "    def generate_enc_mask_m(self, src) :       \n",
    "        mask_m = (src != train_dataset.prep.pad_id).unsqueeze(1).unsqueeze(2)\n",
    "        return mask_m\n",
    "    \n",
    "    def forward(self, txt, seg) : \n",
    "        emb = self.tok_emb(txt)\n",
    "        pos = torch.arange(0, emb.shape[1]).unsqueeze(0).repeat(emb.shape[0], 1).to(emb.device)\n",
    "        summed = emb / math.sqrt(self.d_model) + self.pos_emb(pos) + self.seg_emb(seg)\n",
    "        return summed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.880545Z",
     "start_time": "2022-01-30T14:18:18.874078Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module) : \n",
    "    def __init__(self, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask) :         \n",
    "        score = torch.matmul(q, k.permute(0,1,3,2).contiguous())/math.sqrt(d_model)\n",
    "        score = score.masked_fill(mask, -1e10)\n",
    "        scaled_score = torch.softmax(score, dim=-1)\n",
    "        \n",
    "        attention = torch.matmul(scaled_score, v).permute(0,2,3,1).contiguous()\n",
    "        attention = attention.view(attention.shape[0], attention.shape[1], self.d_model)\n",
    "        \n",
    "        return self.fc(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.891465Z",
     "start_time": "2022-01-30T14:18:18.882601Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) : \n",
    "    def __init__(self, d_model, seq_length, n_head) : \n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, f\"n_head({n_head}) does not divide d_model({d_model})\"\n",
    "\n",
    "        self.n_div_head = d_model//n_head\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_length\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.Q = nn.Linear(d_model,  d_model)\n",
    "        self.K = nn.Linear(d_model,  d_model)\n",
    "        self.V = nn.Linear(d_model,  d_model)\n",
    "        \n",
    "    def div_and_sort_for_multiheads(self, projected, seq_len) : \n",
    "        div = projected.view(projected.shape[0], self.n_head, seq_len, self.n_div_head)\n",
    "        return div\n",
    "    \n",
    "    def forward(self, emb, enc_inputs=None) :\n",
    "        q = self.div_and_sort_for_multiheads(self.Q(emb), self.seq_len)\n",
    "    \n",
    "        if enc_inputs is not None : # enc-dec attention\n",
    "            seq_len = enc_inputs.shape[1] # takes target sequence length for k and v\n",
    "            k = self.div_and_sort_for_multiheads(self.K(enc_inputs), seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(enc_inputs), seq_len)\n",
    "        else : # self-attention\n",
    "            k = self.div_and_sort_for_multiheads(self.K(emb), self.seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(emb), self.seq_len)\n",
    "\n",
    "        return q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process the sub-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.898017Z",
     "start_time": "2022-01-30T14:18:18.893404Z"
    }
   },
   "outputs": [],
   "source": [
    "class PostProcessing(nn.Module) : \n",
    "    def __init__(self, d_model, p=0.1) : \n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, emb, attn) : \n",
    "        return self.ln(emb+self.dropout(attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.904269Z",
     "start_time": "2022-01-30T14:18:18.899938Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module) : \n",
    "    def __init__(self, d_model, d_ff) : \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        return self.fc2(self.act(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.913985Z",
     "start_time": "2022-01-30T14:18:18.907122Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model, d_ff, n_head, dropout_p) : \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ma = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.sdp = ScaledDotProductAttention(d_model)\n",
    "        \n",
    "        self.pp1 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp2 = PostProcessing(d_model, dropout_p)\n",
    "        \n",
    "        self.positionwise_ffn = PositionwiseFFN(d_model, d_ff)\n",
    "            \n",
    "    def forward(self, emb, mask_m) :\n",
    "\n",
    "        q,k,v = self.ma(emb)    \n",
    "        attn = self.sdp(q,k,v, mask=mask_m)\n",
    "        \n",
    "        attn = self.pp1(emb, attn)\n",
    "        z = self.positionwise_ffn(attn)\n",
    "\n",
    "        return self.pp2(attn, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:18.932506Z",
     "start_time": "2022-01-30T14:18:18.917760Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module) : \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 seq_length,\n",
    "                 d_model,\n",
    "                 d_ff,\n",
    "                 n_head,\n",
    "                 dropout_p,\n",
    "                 n_enc_layer) : \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embber = InputEmbedding(vocab_size, seq_length, d_model)\n",
    "        \n",
    "        enc = EncoderLayer(vocab_size, seq_length, d_model, d_ff, n_head, dropout_p)\n",
    "        \n",
    "        self.enc = nn.ModuleList([deepcopy(enc) for _ in range(n_enc_layer)])\n",
    "        \n",
    "        self.mlm_fc = nn.Linear(d_model, vocab_size)\n",
    "        self.nsp_fc = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, txt, seg) : \n",
    "        \n",
    "        emb = self.embber(txt, seg)\n",
    "        mask_m = self.embber.generate_enc_mask_m(txt)\n",
    "\n",
    "        for enc_layer in self.enc : \n",
    "            emb = enc_layer(emb, mask_m)\n",
    "        \n",
    "        return emb, self.mlm_fc(emb), self.nsp_fc(emb[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:25.865424Z",
     "start_time": "2022-01-30T14:18:18.936530Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_len = 128\n",
    "\n",
    "train_dataset = iterator.BertIterator(filename = '../data/prep_train.txt',\n",
    "                                tokenizer_model_path = '../data/m.model',\n",
    "                                seq_len=seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = iterator.BertIterator(filename = '../data/prep_valid.txt',\n",
    "                                tokenizer_model_path = '../data/m.model',\n",
    "                                seq_len=seq_len)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:26.118864Z",
     "start_time": "2022-01-30T14:18:25.869229Z"
    }
   },
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "d_ff = d_model * 4\n",
    "n_head = 8\n",
    "vocab_size = train_dataset.prep.vocab_size\n",
    "dropout_p = 0.1\n",
    "n_enc_layer = 3\n",
    "seq_length = train_dataset.seq_len\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:18:30.733630Z",
     "start_time": "2022-01-30T14:18:26.123776Z"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.DataParallel(BERT(vocab_size,\n",
    "             seq_length,\n",
    "             d_model,\n",
    "             d_ff,\n",
    "             n_head,\n",
    "             dropout_p,\n",
    "             n_enc_layer)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader : \n",
    "    emb, mlm_pred, nsp_pred = model(data['text'].to(device), data['seg'].to(device))\n",
    "    cri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:16:43.384326Z",
     "start_time": "2022-01-30T14:16:43.377022Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True) # -- for debugging the train progress --\n",
    "N_EPOCHS = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, pct_start=0.01, \n",
    "                                          steps_per_epoch=len(train_dataloader), epochs=N_EPOCHS, \n",
    "                                          total_steps=N_EPOCHS * len(train_dataloader), anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:17:13.946269Z",
     "start_time": "2022-01-30T14:17:13.916669Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, scheduler):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_total_loss = 0\n",
    "    epoch_mlm_loss = 0\n",
    "    epoch_nsp_loss = 0 \n",
    "    \n",
    "    epoch_mlm_acc = 0    \n",
    "    epoch_nsp_acc = 0    \n",
    "    cnt = 0\n",
    "    \n",
    "    for data in tqdm(dataloader, desc='train') :        \n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        emb, mlm_pred, nsp_pred = model(data['text'].to(device), data['seg'].to(device))\n",
    "        \n",
    "#         mlm_loss = criterion(mlm_pred.view(-1,vocab_size), data['mlm'].cuda().view(-1))\n",
    "#         mlm_loss = criterion(mlm_pred.transpose(1,2), data['mlm'].cuda())\n",
    "        nsp_loss = criterion(nsp_pred, data['nsp'].cuda()) \n",
    "        loss = nsp_loss\n",
    "#         loss = mlm_loss + nsp_loss\n",
    "        \n",
    "        loss.backward()\n",
    "#        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        \n",
    "        optimizer.step()        \n",
    "        \n",
    "#         acc = (mlm_pred.view(-1,vocab_size).argmax(1) == data['mlm'].view(-1).cuda()).sum().item() / data['mlm'].view(-1).shape[0]\n",
    "#         epoch_mlm_acc += acc\n",
    "        \n",
    "        acc = (nsp_pred.argmax(1) == data['nsp'].cuda()).sum() / data['nsp'].shape[0]\n",
    "        epoch_nsp_acc += acc\n",
    "        \n",
    "        epoch_total_loss += loss.item()\n",
    "#         epoch_mlm_loss += mlm_loss.item()\n",
    "        epoch_nsp_loss += nsp_loss.item()        \n",
    "        cnt += 1\n",
    "        \n",
    "#         if cnt % (len(dataloader)//1000) == 0 : \n",
    "#             print(f'\\tTrain Total Loss: {epoch_total_loss / cnt:.3f} | Train MLM Loss: {epoch_mlm_loss / cnt:.3f} | Train NSP Loss: {epoch_nsp_loss / cnt:.3f}\\\n",
    "#             | MLM ACC : {epoch_mlm_acc / cnt: .3f} | NSP ACC : {epoch_nsp_acc / cnt: .3f} | Learning Rate : {scheduler.get_last_lr()[0]:.3f}')\n",
    "\n",
    "        if cnt % (len(dataloader)//1000) == 0 : \n",
    "            print(f'\\tTrain Total Loss: {epoch_total_loss / cnt:.3f} | Train MLM Loss: {epoch_mlm_loss / cnt:.3f} | Train NSP Loss: {epoch_nsp_loss / cnt:.3f}\\\n",
    "            | NSP ACC : {epoch_nsp_acc / cnt: .3f} | Learning Rate : {scheduler.get_last_lr()[0]:.3f}')\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    return epoch_total_loss / cnt, epoch_mlm_loss / cnt, epoch_nsp_loss / cnt\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_total_loss = 0\n",
    "    epoch_mlm_loss = 0\n",
    "    epoch_nsp_loss = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    with torch.no_grad() : \n",
    "        for data in tqdm(data, desc='valid') :\n",
    "            \n",
    "            emb, mlm_pred, nsp_pred = model(data['text'].to(device), data['seg'].to(device))\n",
    "        \n",
    "#             mlm_loss = criterion(mlm_pred.view(-1,vocab_size), data['mlm'].cuda().view(-1))\n",
    "            mlm_loss = criterion(mlm_pred.transpose(1,2), data['mlm'].cuda())\n",
    "            nsp_loss = criterion(nsp_pred, data['nsp'].cuda()) \n",
    "            loss = mlm_loss + nsp_loss\n",
    "\n",
    "            epoch_total_loss += loss.item()\n",
    "            epoch_mlm_loss += mlm_loss.item()\n",
    "            epoch_nsp_loss += nsp_loss.item()        \n",
    "            cnt += 1\n",
    "    \n",
    "        if cnt % (len(dataloader)//1000) == 0 : \n",
    "            print(f'\\tTrain Total Loss: {epoch_total_loss / cnt:.3f} | Train MLM Loss: {epoch_mlm_loss / cnt:.3f} | Train NSP Loss: {epoch_nsp_loss / cnt:.3f}')\n",
    "            \n",
    "    return epoch_total_loss / cnt, epoch_mlm_loss / cnt, epoch_nsp_loss / cnt\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:17:43.141605Z",
     "start_time": "2022-01-30T14:17:14.878292Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dbc58edc644c73b8ca8fa505962ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/26566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 0.000 | Train MLM Loss: 0.000 | Train NSP Loss: 0.000            | NSP ACC :  0.504 | Learning Rate : 0.004\n",
      "\tTrain Total Loss: 0.000 | Train MLM Loss: 0.000 | Train NSP Loss: 0.000            | NSP ACC :  0.490 | Learning Rate : 0.005\n",
      "\tTrain Total Loss: 0.000 | Train MLM Loss: 0.000 | Train NSP Loss: 0.000            | NSP ACC :  0.493 | Learning Rate : 0.005\n",
      "\tTrain Total Loss: 0.001 | Train MLM Loss: 0.000 | Train NSP Loss: 0.001            | NSP ACC :  0.492 | Learning Rate : 0.006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-4c9e7c57c59b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mlm_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nsp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalid_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_mlm_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_nsp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-2ed129f1e4ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, scheduler)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         mlm_loss = criterion(mlm_pred.view(-1,vocab_size), data['mlm'].cuda().view(-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         mlm_loss = criterion(mlm_pred.transpose(1,2), data['mlm'].cuda())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mnsp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsp_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nsp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnsp_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         loss = mlm_loss + nsp_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_total_loss, train_mlm_loss, train_nsp_loss = train(model, train_dataloader, optimizer, criterion, scheduler)\n",
    "    valid_total_loss, valid_mlm_loss, valid_nsp_loss = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    end_time = time.time()    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_total_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_total_loss\n",
    "        torch.save(model.state_dict(), 'bert.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    \n",
    "    print(f'\\tTrain Total Loss: {train_total_loss:.3f} | Train MLM Loss: {train_mlm_loss:.3f} | Train NSP Loss: {train_nsp_loss:.3f}')\n",
    "    \n",
    "    print(f'\\tValid Total Loss: {valid_total_loss:.3f} | Valid MLM Loss: {valid_mlm_loss:.3f} | Valid NSP Loss: {valid_nsp_loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
