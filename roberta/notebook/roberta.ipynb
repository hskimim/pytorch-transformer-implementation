{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:24:03.896638Z",
     "start_time": "2022-02-02T12:24:02.454908Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:24:03.904733Z",
     "start_time": "2022-02-02T12:24:03.899507Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:24:03.917538Z",
     "start_time": "2022-02-02T12:24:03.907205Z"
    }
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=train_dataset.prep.pad_id)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d_model, padding_idx=train_dataset.prep.pad_id)\n",
    "        \n",
    "    def generate_enc_mask_m(self, src) :       \n",
    "        mask_m = (src != 1).unsqueeze(1).unsqueeze(2)\n",
    "        return mask_m\n",
    "    \n",
    "    def forward(self, txt) : \n",
    "        emb = self.tok_emb(txt)\n",
    "        pos = torch.arange(0, emb.shape[1]).unsqueeze(0).repeat(emb.shape[0], 1).to(emb.device)\n",
    "        summed = emb / math.sqrt(self.d_model) + self.pos_emb(pos)\n",
    "        return summed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:24:03.932664Z",
     "start_time": "2022-02-02T12:24:03.921541Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module) : \n",
    "    def __init__(self, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask) :         \n",
    "        score = torch.matmul(q, k.permute(0,1,3,2).contiguous())/math.sqrt(d_model)\n",
    "        score = score.masked_fill(mask, -1e10)\n",
    "        scaled_score = torch.softmax(score, dim=-1)\n",
    "        \n",
    "        attention = torch.matmul(scaled_score, v).permute(0,2,3,1).contiguous()\n",
    "        attention = attention.view(attention.shape[0], attention.shape[1], self.d_model)\n",
    "        \n",
    "        return self.fc(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:24:03.949487Z",
     "start_time": "2022-02-02T12:24:03.936699Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) : \n",
    "    def __init__(self, d_model, seq_length, n_head) : \n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, f\"n_head({n_head}) does not divide d_model({d_model})\"\n",
    "\n",
    "        self.n_div_head = d_model//n_head\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_length\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.Q = nn.Linear(d_model,  d_model)\n",
    "        self.K = nn.Linear(d_model,  d_model)\n",
    "        self.V = nn.Linear(d_model,  d_model)\n",
    "        \n",
    "    def div_and_sort_for_multiheads(self, projected, seq_len) : \n",
    "        div = projected.view(projected.shape[0], self.n_head, seq_len, self.n_div_head)\n",
    "        return div\n",
    "    \n",
    "    def forward(self, emb, enc_inputs=None) :\n",
    "        q = self.div_and_sort_for_multiheads(self.Q(emb), self.seq_len)\n",
    "    \n",
    "        if enc_inputs is not None : # enc-dec attention\n",
    "            seq_len = enc_inputs.shape[1] # takes target sequence length for k and v\n",
    "            k = self.div_and_sort_for_multiheads(self.K(enc_inputs), seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(enc_inputs), seq_len)\n",
    "        else : # self-attention\n",
    "            k = self.div_and_sort_for_multiheads(self.K(emb), self.seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(emb), self.seq_len)\n",
    "\n",
    "        return q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process the sub-layer\n",
    "- layer normalization\n",
    "- residual conection\n",
    "- residual dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:28:21.042405Z",
     "start_time": "2022-02-02T12:28:21.032506Z"
    }
   },
   "outputs": [],
   "source": [
    "class PostProcessing(nn.Module) : \n",
    "    def __init__(self, d_model, p=0.1) : \n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, emb, attn) : \n",
    "        return emb+self.dropout(self.ln(attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:24:03.966134Z",
     "start_time": "2022-02-02T12:24:03.960302Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module) : \n",
    "    def __init__(self, d_model, d_ff) : \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        return self.fc2(torch.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:24:03.978479Z",
     "start_time": "2022-02-02T12:24:03.970277Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model, d_ff, n_head, dropout_p) : \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ma = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.sdp = ScaledDotProductAttention(d_model)\n",
    "        \n",
    "        self.pp1 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp2 = PostProcessing(d_model, dropout_p)\n",
    "        \n",
    "        self.positionwise_ffn = PositionwiseFFN(d_model, d_ff)\n",
    "            \n",
    "    def forward(self, emb, mask_m) :\n",
    "\n",
    "        q,k,v = self.ma(emb)    \n",
    "        attn = self.sdp(q,k,v, mask=mask_m)\n",
    "        \n",
    "        attn = self.pp1(emb, attn)\n",
    "        z = self.positionwise_ffn(attn)\n",
    "\n",
    "        return self.pp2(attn, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:28:23.841825Z",
     "start_time": "2022-02-02T12:28:23.831209Z"
    }
   },
   "outputs": [],
   "source": [
    "class ROBERTa(nn.Module) : \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 seq_length,\n",
    "                 d_model,\n",
    "                 d_ff,\n",
    "                 n_head,\n",
    "                 dropout_p,\n",
    "                 n_enc_layer) : \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embber = InputEmbedding(vocab_size, seq_length, d_model)\n",
    "        \n",
    "        enc = EncoderLayer(vocab_size, seq_length, d_model, d_ff, n_head, dropout_p)\n",
    "        \n",
    "        self.enc = nn.ModuleList([deepcopy(enc) for _ in range(n_enc_layer)])\n",
    "\n",
    "    def forward(self, txt) : \n",
    "        \n",
    "        emb = self.embber(txt)\n",
    "        mask_m = self.embber.generate_enc_mask_m(txt)\n",
    "\n",
    "        for enc_layer in self.enc : \n",
    "            emb = enc_layer(emb, mask_m)\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:28:23.861679Z",
     "start_time": "2022-02-02T12:28:23.852511Z"
    }
   },
   "outputs": [],
   "source": [
    "class ROBERTaFC(nn.Module) : \n",
    "    def __init__(self, embedder, d_model, vocab_size) : \n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.mlm_fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, txt) : \n",
    "        emb = self.embedder(txt)\n",
    "        return torch.log_softmax(self.mlm_fc(emb), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:28:25.663315Z",
     "start_time": "2022-02-02T12:28:25.344033Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "seq_len = 256\n",
    "\n",
    "train_dataset = iterator.RobertaIterator(filename = '../../bert/data/wikitext-2-raw/prep_train.txt',\n",
    "                                tokenizer_model_path = '../../bert/data/wiki_pretrained_vocab/m.model',\n",
    "                                seq_len=seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    " \n",
    "valid_dataset = iterator.RobertaIterator(filename = '../../bert/data/wikitext-2-raw/prep_valid.txt',\n",
    "                                tokenizer_model_path = '../../bert/data/wiki_pretrained_vocab/m.model',\n",
    "                                seq_len=seq_len)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:28:26.140466Z",
     "start_time": "2022-02-02T12:28:26.129584Z"
    }
   },
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "d_ff = d_model * 4\n",
    "n_head = 8\n",
    "vocab_size = 30000\n",
    "dropout_p = 0.1\n",
    "n_enc_layer = 1\n",
    "seq_length = train_dataset.seq_len\n",
    "vocab = train_dataset.vocab\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:28:26.422127Z",
     "start_time": "2022-02-02T12:28:26.287656Z"
    }
   },
   "outputs": [],
   "source": [
    "roberta_ember = ROBERTa(vocab_size,\n",
    "             seq_length,\n",
    "             d_model,\n",
    "             d_ff,\n",
    "             n_head,\n",
    "             dropout_p,\n",
    "             n_enc_layer).to(device) \n",
    "# generate embeding using transformer architecture\n",
    "\n",
    "roberta_predicter = nn.DataParallel(ROBERTaFC(roberta_ember, d_model, vocab_size)).to(device)\n",
    "# return 2 fc layer for training bi-directional representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:28:27.565208Z",
     "start_time": "2022-02-02T12:28:27.553859Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True) # -- for debugging the train progress --\n",
    "N_EPOCHS = 20\n",
    "\n",
    "criterion1 = nn.NLLLoss(ignore_index=0).to(device)\n",
    "criterion2 = nn.NLLLoss().to(device)\n",
    "\n",
    "optimizer = optim.AdamW(roberta_predicter.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-1, pct_start=0.01, \n",
    "                                          steps_per_epoch=len(train_dataloader), epochs=N_EPOCHS, \n",
    "                                          total_steps=N_EPOCHS * len(train_dataloader), anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T12:28:27.807542Z",
     "start_time": "2022-02-02T12:28:27.779980Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, scheduler):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_total_loss = 0\n",
    "    epoch_mlm_acc = 0    \n",
    "    cnt = 0\n",
    "    \n",
    "    for data in tqdm(dataloader, desc='train') :\n",
    "        \n",
    "        data = {k:v.to(device) for k,v in data.items()}\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        mlm_pred = model(data['text'])\n",
    "\n",
    "        # calculate loss from masked language modeling\n",
    "        loss = criterion1(mlm_pred.transpose(1,2), data['mlm'])\n",
    "                \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()        \n",
    "        \n",
    "        # calculate acc for mlm\n",
    "        acc = (mlm_pred.view(-1,vocab_size).argmax(1) == data['mlm'].view(-1)).sum().item() / data['mlm'].view(-1).shape[0]\n",
    "        epoch_mlm_acc += acc\n",
    "        \n",
    "        epoch_total_loss += loss.item()\n",
    "        cnt += 1\n",
    "        scheduler.step()\n",
    "        \n",
    "    print(f'\\tTrain Total Loss: {epoch_total_loss / cnt:.3f} | MLM ACC : {epoch_mlm_acc / cnt: .3f} | Learning Rate : {scheduler.get_last_lr()[0]:.3f}')\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_total_loss = 0\n",
    "    epoch_mlm_acc = 0    \n",
    "    cnt = 0\n",
    "    \n",
    "    with torch.no_grad() : \n",
    "        for data in tqdm(dataloader, desc='valid') :\n",
    "            \n",
    "            data = {k:v.to(device) for k,v in data.items()}\n",
    "\n",
    "            mlm_pred = model(data['text'])\n",
    "\n",
    "            # calculate loss from masked language modeling\n",
    "            loss = criterion1(mlm_pred.transpose(1,2), data['mlm'])\n",
    "            \n",
    "            acc = (mlm_pred.view(-1,vocab_size).argmax(1) == data['mlm'].view(-1)).sum().item() / data['mlm'].view(-1).shape[0]\n",
    "            epoch_mlm_acc += acc\n",
    "\n",
    "            epoch_total_loss += loss.item()\n",
    "            cnt += 1\n",
    "\n",
    "        print(f'\\tValid Total Loss: {epoch_total_loss / cnt:.3f} | MLM ACC : {epoch_mlm_acc / cnt: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T13:10:56.876630Z",
     "start_time": "2022-02-02T12:28:28.291571Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21e43aecc4740418cf48bfcfad8006c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 7.304 | MLM ACC :  0.011 | Learning Rate : 0.096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f93a35033294abcbe27ae63caa5a366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.865 | MLM ACC :  0.015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5657339c856747ef8db4a605d8b7142e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.883 | MLM ACC :  0.016 | Learning Rate : 0.091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4e717b5fec4b5daa4f00e3e174997e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.774 | MLM ACC :  0.016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ea2b1b70ca4ec68f5ef17b8f49845f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.821 | MLM ACC :  0.016 | Learning Rate : 0.086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fb823b23cc4f0cb973d911485d4745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.702 | MLM ACC :  0.017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5030af969c84deea6e4e042a3349c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.753 | MLM ACC :  0.017 | Learning Rate : 0.081\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfaa8e04107458cb89577419216896b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.641 | MLM ACC :  0.017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a32f9e25714f669a17943cdc69f493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.718 | MLM ACC :  0.017 | Learning Rate : 0.076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fc85be29d440d4abbcf247b112c425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.632 | MLM ACC :  0.017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0931505a66044e6af36febf593f215e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.701 | MLM ACC :  0.017 | Learning Rate : 0.071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7095b8b1caf474f816bc43c68efb219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.612 | MLM ACC :  0.017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75898622a854034869c3141a39517c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.668 | MLM ACC :  0.017 | Learning Rate : 0.066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7d367aa8354724a7fb1e1ae7c8a7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.588 | MLM ACC :  0.017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd0775ecdd64a4c90d489401de4dc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.660 | MLM ACC :  0.017 | Learning Rate : 0.061\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81d1938c57b43c0b33984c4ca787664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.569 | MLM ACC :  0.017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec294917ba54ff782ed1943c298bb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.627 | MLM ACC :  0.018 | Learning Rate : 0.056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981affcad3e3450ea7fe842156d3086b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.546 | MLM ACC :  0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56833003d0f34c52a8dbc02391da50ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.601 | MLM ACC :  0.018 | Learning Rate : 0.050\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc2de7e2f394d6691aaf5bdce983729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.529 | MLM ACC :  0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f5e4fbf941493d8c3689bd9d9789c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.579 | MLM ACC :  0.018 | Learning Rate : 0.045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52134668ef94ad9bd6c9da081bf89a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.505 | MLM ACC :  0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8f9a47230e46ce8da05343815f780b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.546 | MLM ACC :  0.018 | Learning Rate : 0.040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb08cac479244a69ebc82b365aa370e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.487 | MLM ACC :  0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33a340f71a048ff9e1d8b55ec2436e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.515 | MLM ACC :  0.018 | Learning Rate : 0.035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397fd40b01164168b208a7ed30500cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.467 | MLM ACC :  0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d81284a4ad0471f93ae8685ec090c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.487 | MLM ACC :  0.018 | Learning Rate : 0.030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cbb87ec56f4df9b16287662c2bdb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.450 | MLM ACC :  0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360a739734e0409ca522a284e957cf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.459 | MLM ACC :  0.019 | Learning Rate : 0.025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015dcf20737846c9892dfe74e4b91ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.428 | MLM ACC :  0.019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc77eda775af4e498e5bac05ef862c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.442 | MLM ACC :  0.019 | Learning Rate : 0.020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4464269e7f491ea03d55a9cac92222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.400 | MLM ACC :  0.019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74d146c49ff473bb453d589bdb7fa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.414 | MLM ACC :  0.019 | Learning Rate : 0.015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560cf9458328459ca8d73b70de663881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.386 | MLM ACC :  0.019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc96189c5d24fafb581b515171378d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.394 | MLM ACC :  0.019 | Learning Rate : 0.010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681102de53014e279d78c25e590135bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.357 | MLM ACC :  0.019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fdbd8e77f44bc1887d87dc4f218941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.360 | MLM ACC :  0.019 | Learning Rate : 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8106185922de423baab5c5e26c4464c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.343 | MLM ACC :  0.019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53680cdcaf0d4449a652ea59fc4915d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 6.337 | MLM ACC :  0.020 | Learning Rate : -0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0203a13f1eba48339db6916bd0247d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 6.326 | MLM ACC :  0.020\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):    \n",
    "    train(roberta_predicter, train_dataloader, optimizer, scheduler)\n",
    "    evaluate(roberta_predicter, train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
