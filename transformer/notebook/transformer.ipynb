{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:28:57.062520Z",
     "start_time": "2022-01-30T15:28:52.901023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:28:57.070838Z",
     "start_time": "2022-01-30T15:28:57.065532Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "- copied from https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:01.143075Z",
     "start_time": "2022-01-30T15:28:57.073898Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:01.150387Z",
     "start_time": "2022-01-30T15:29:01.145955Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:10.808964Z",
     "start_time": "2022-01-30T15:29:01.152796Z"
    }
   },
   "outputs": [],
   "source": [
    "src_seq_len = 30\n",
    "trg_seq_len = 30-1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            fix_length=src_seq_len,\n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            fix_length=src_seq_len,\n",
    "            batch_first = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)\n",
    "\n",
    "for i in train_iterator : \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:10.820226Z",
     "start_time": "2022-01-30T15:29:10.811723Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d_model)\n",
    "    \n",
    "    def generate_enc_mask_m(self, src) :       \n",
    "        mask_m = (src != 1).unsqueeze(1).unsqueeze(2)\n",
    "        return mask_m\n",
    "\n",
    "    def generate_dec_mask_m(self, trg) :\n",
    "        trg_pad_mask = (trg != 1).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len)), diagonal=0).bool().to(trg_pad_mask.device)\n",
    "        mask_m = trg_pad_mask & trg_sub_mask\n",
    "        return mask_m\n",
    "    \n",
    "    def forward(self, x) : \n",
    "        emb = self.tok_emb(x)\n",
    "        pos = torch.arange(0, emb.shape[1]).unsqueeze(0).repeat(emb.shape[0], 1).to(emb.device)\n",
    "        summed = emb / math.sqrt(self.d_model) + self.pos_emb(pos)\n",
    "        return summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:32:03.981503Z",
     "start_time": "2022-01-30T15:32:03.880609Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in train_iterator : \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:27.257279Z",
     "start_time": "2022-01-30T15:29:27.243364Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module) : \n",
    "    def __init__(self, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask) :         \n",
    "        score = torch.matmul(q, k.permute(0,1,3,2).contiguous())/math.sqrt(d_model)\n",
    "        score = score.masked_fill(mask==0, -1e10)\n",
    "        scaled_score = torch.softmax(score, dim=-1)\n",
    "        \n",
    "        attention = torch.matmul(scaled_score, v).permute(0,2,3,1).contiguous()\n",
    "        attention = attention.view(attention.shape[0], attention.shape[1], self.d_model)\n",
    "        \n",
    "        return self.fc(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:28.629984Z",
     "start_time": "2022-01-30T15:29:28.607492Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) : \n",
    "    def __init__(self, d_model, seq_length, n_head) : \n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, f\"n_head({n_head}) does not divide d_model({d_model})\"\n",
    "\n",
    "        self.n_div_head = d_model//n_head\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_length\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.Q = nn.Linear(d_model,  d_model)\n",
    "        self.K = nn.Linear(d_model,  d_model)\n",
    "        self.V = nn.Linear(d_model,  d_model)\n",
    "        \n",
    "    def div_and_sort_for_multiheads(self, projected, seq_len) : \n",
    "        div = projected.view(projected.shape[0], self.n_head, seq_len, self.n_div_head)\n",
    "        return div\n",
    "    \n",
    "    def forward(self, emb, enc_inputs=None) :\n",
    "        q = self.div_and_sort_for_multiheads(self.Q(emb), self.seq_len)\n",
    "    \n",
    "        if enc_inputs is not None : # enc-dec attention\n",
    "            seq_len = enc_inputs.shape[1] # takes target sequence length for k and v\n",
    "            k = self.div_and_sort_for_multiheads(self.K(enc_inputs), seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(enc_inputs), seq_len)\n",
    "        else : # self-attention\n",
    "            k = self.div_and_sort_for_multiheads(self.K(emb), self.seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(emb), self.seq_len)\n",
    "\n",
    "        return q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process the sub-layer\n",
    "- layer normalization\n",
    "- residual conection\n",
    "- residual dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:30.293642Z",
     "start_time": "2022-01-30T15:29:30.282483Z"
    }
   },
   "outputs": [],
   "source": [
    "class PostProcessing(nn.Module) : \n",
    "    def __init__(self, d_model, p=0.1) : \n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, emb, attn) : \n",
    "        return self.ln(emb+self.dropout(attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:31.038096Z",
     "start_time": "2022-01-30T15:29:31.028816Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module) : \n",
    "    def __init__(self, d_model, d_ff) : \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        return self.fc2(torch.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:31.694273Z",
     "start_time": "2022-01-30T15:29:31.680018Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model, d_ff, n_head, dropout_p) : \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ma = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.sdp = ScaledDotProductAttention(d_model)\n",
    "        \n",
    "        self.pp1 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp2 = PostProcessing(d_model, dropout_p)\n",
    "        \n",
    "        self.positionwise_ffn = PositionwiseFFN(d_model, d_ff)\n",
    "            \n",
    "    def forward(self, emb, mask_m) :\n",
    "\n",
    "        q,k,v = self.ma(emb)    \n",
    "        attn = self.sdp(q,k,v, mask=mask_m)\n",
    "        \n",
    "        attn = self.pp1(emb, attn)\n",
    "        z = self.positionwise_ffn(attn)\n",
    "\n",
    "        return self.pp2(attn, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:29:35.064367Z",
     "start_time": "2022-01-30T15:29:35.046311Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model, d_ff, n_head, dropout_p) : \n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.ma_self = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.ma_enc_dec = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.sdp_self = ScaledDotProductAttention(d_model)\n",
    "        self.sdp_enc_dec = ScaledDotProductAttention(d_model)\n",
    "        \n",
    "        self.pp1 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp2 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp3 = PostProcessing(d_model, dropout_p)\n",
    "\n",
    "        self.positionwise_ffn = PositionwiseFFN(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, emb, mask_m_src, mask_m_trg, enc_hidden) : \n",
    "        \n",
    "        q,k,v = self.ma_self(emb)\n",
    "        attn = self.sdp_self(q,k,v, mask=mask_m_trg)\n",
    "        attn1 = self.pp1(emb, attn)\n",
    "        \n",
    "        dec_q,enc_k,enc_v = self.ma_enc_dec(attn1, enc_hidden)\n",
    "        attn2 = self.sdp_enc_dec(dec_q, enc_k, enc_v, mask_m_src)\n",
    "        sub_layer_output = self.pp2(attn1, attn2)\n",
    "\n",
    "        z = self.positionwise_ffn(sub_layer_output)\n",
    "\n",
    "        return self.pp3(sub_layer_output, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:30:01.324207Z",
     "start_time": "2022-01-30T15:30:01.303490Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module) : \n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 trg_vocab_size,\n",
    "                 src_seq_length,\n",
    "                 trg_seq_length,\n",
    "                 d_model,\n",
    "                 d_ff,\n",
    "                 n_head,\n",
    "                 dropout_p,\n",
    "                 n_enc_layer,\n",
    "                 n_dec_layer) : \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.src_embber = InputEmbedding(src_vocab_size, src_seq_length, d_model)\n",
    "        self.trg_embber = InputEmbedding(trg_vocab_size, trg_seq_length, d_model)\n",
    "        \n",
    "        enc = EncoderLayer(src_vocab_size, src_seq_length, d_model, d_ff, n_head, dropout_p)\n",
    "        dec = DecoderLayer(trg_vocab_size, trg_seq_length, d_model, d_ff, n_head, dropout_p)\n",
    "        \n",
    "        self.enc = nn.ModuleList([deepcopy(enc) for _ in range(n_enc_layer)])\n",
    "        self.dec = nn.ModuleList([deepcopy(dec) for _ in range(n_dec_layer)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, src, trg) : \n",
    "        \n",
    "        src_emb, trg_emb = self.src_embber(src), self.trg_embber(trg)\n",
    "        src_mask_m = self.src_embber.generate_enc_mask_m(src)\n",
    "        trg_mask_m = self.trg_embber.generate_dec_mask_m(trg)\n",
    "        \n",
    "        for enc_layer in self.enc : \n",
    "            src_emb = enc_layer(src_emb, src_mask_m)\n",
    "        \n",
    "        for dec_layer in self.dec : \n",
    "            trg_emb = dec_layer(trg_emb, src_mask_m, trg_mask_m, src_emb)\n",
    "        \n",
    "        return self.fc(trg_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:34:20.578418Z",
     "start_time": "2022-01-30T15:34:20.569957Z"
    }
   },
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "d_ff = 512\n",
    "n_head = 8\n",
    "batch_size = BATCH_SIZE\n",
    "src_vocab_size = len(SRC.vocab)\n",
    "trg_vocab_size = len(TRG.vocab)\n",
    "dropout_p = 0.1\n",
    "n_enc_layer, n_dec_layer = 3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:34:20.960418Z",
     "start_time": "2022-01-30T15:34:20.721326Z"
    }
   },
   "outputs": [],
   "source": [
    "model = EncoderDecoder(src_vocab_size,\n",
    "                         trg_vocab_size,\n",
    "                         src_seq_len,\n",
    "                         trg_seq_len,\n",
    "                         d_model,\n",
    "                         d_ff,\n",
    "                         n_head,\n",
    "                         dropout_p,\n",
    "                         n_enc_layer,\n",
    "                         n_dec_layer).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:34:22.694662Z",
     "start_time": "2022-01-30T15:34:22.688291Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:34:22.870599Z",
     "start_time": "2022-01-30T15:34:22.850990Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator) :\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:,:-1])                \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg[:,:-1])            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:39:45.419960Z",
     "start_time": "2022-01-30T15:34:23.392277Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 31s\n",
      "\tTrain Loss: 4.867 | Train PPL: 129.942\n",
      "\t Val. Loss: 4.400 |  Val. PPL:  81.477\n",
      "Epoch: 02 | Time: 0m 31s\n",
      "\tTrain Loss: 4.331 | Train PPL:  76.014\n",
      "\t Val. Loss: 3.822 |  Val. PPL:  45.709\n",
      "Epoch: 03 | Time: 0m 32s\n",
      "\tTrain Loss: 3.560 | Train PPL:  35.158\n",
      "\t Val. Loss: 3.117 |  Val. PPL:  22.573\n",
      "Epoch: 04 | Time: 0m 32s\n",
      "\tTrain Loss: 3.045 | Train PPL:  21.003\n",
      "\t Val. Loss: 2.754 |  Val. PPL:  15.710\n",
      "Epoch: 05 | Time: 0m 32s\n",
      "\tTrain Loss: 2.763 | Train PPL:  15.848\n",
      "\t Val. Loss: 2.521 |  Val. PPL:  12.439\n",
      "Epoch: 06 | Time: 0m 32s\n",
      "\tTrain Loss: 2.560 | Train PPL:  12.940\n",
      "\t Val. Loss: 2.362 |  Val. PPL:  10.609\n",
      "Epoch: 07 | Time: 0m 32s\n",
      "\tTrain Loss: 2.416 | Train PPL:  11.196\n",
      "\t Val. Loss: 2.171 |  Val. PPL:   8.765\n",
      "Epoch: 08 | Time: 0m 32s\n",
      "\tTrain Loss: 2.266 | Train PPL:   9.642\n",
      "\t Val. Loss: 2.165 |  Val. PPL:   8.712\n",
      "Epoch: 09 | Time: 0m 32s\n",
      "\tTrain Loss: 2.143 | Train PPL:   8.525\n",
      "\t Val. Loss: 1.971 |  Val. PPL:   7.181\n",
      "Epoch: 10 | Time: 0m 32s\n",
      "\tTrain Loss: 1.999 | Train PPL:   7.381\n",
      "\t Val. Loss: 1.784 |  Val. PPL:   5.956\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:49:39.229731Z",
     "start_time": "2022-01-30T15:49:38.929084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.812 | Test PPL:   6.125 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T15:49:41.098574Z",
     "start_time": "2022-01-30T15:49:40.588413Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['<sos>', 'zwei', 'mittelgroße', 'hunde', 'laufen', 'über', 'den', 'schnee', '.', '<eos>']\n",
      "trg = ['<sos>', 'two', 'medium', 'sized', 'dogs', 'run', 'across', 'the', 'snow', '.']\n",
      "pred = ['two', 'scuba', 'martial', 'dogs', 'up', 'across', 'the', 'snow', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'vier', 'personen', 'spielen', 'fußball', 'auf', 'einem', 'strand', '.', '<eos>']\n",
      "trg = ['<sos>', 'four', 'people', 'are', 'playing', 'soccer', 'on', 'a', 'beach', '.']\n",
      "pred = ['four', 'people', 'are', 'playing', 'water', 'on', 'a', 'beach', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'junge', 'fährt', 'skateboard', 'auf', 'einer', 'skateboardrampe', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'boy', 'riding', 'a', 'skateboard', 'on', 'a', 'skateboarding', 'ramp']\n",
      "pred = ['a', 'boy', 'playing', 'a', 'skateboard', 'on', 'a', 'playground', 'wall']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'hund', 'springt', 'durch', 'ein', 'brennendes', 'hindernis', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'dog', 'is', 'jumping', 'through', 'a', '<unk>', 'obstacle', '.']\n",
      "pred = ['a', 'dog', 'is', 'jumping', 'through', 'a', '<unk>', 'obstacle', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'die', 'zwei', 'kinder', 'spielen', 'auf', 'dem', 'spielplatz', '.', '<eos>']\n",
      "trg = ['<sos>', 'the', 'two', 'kids', 'are', 'playing', 'at', 'the', 'playground', '.']\n",
      "pred = ['the', 'two', 'kids', 'are', 'playing', 'at', 'the', 'ocean', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'mädchen', 'in', 'einem', '<unk>', 'mit', 'offenem', 'mund', '<eos>']\n",
      "trg = ['<sos>', 'girl', 'wearing', 'radio', 't', '-', 'shirt', 'has', 'open', 'mouth']\n",
      "pred = ['girl', 'with', 'hard', 'two', '-', 'boy', 'having', 'snow', 'pool']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'brauner', 'hund', 'läuft', 'über', 'den', 'sandstrand', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'brown', 'dog', 'runs', 'down', 'the', 'sandy', 'beach', '.']\n",
      "pred = ['a', 'little', 'dog', 'runs', 'at', 'the', 'crowded', 'field', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'asiatischer', 'fabrikarbeiter', 'posiert', 'für', 'die', 'kamera', '.', '<eos>']\n",
      "trg = ['<sos>', 'an', 'asian', 'factory', 'worker', 'posing', 'for', 'the', 'camera', '.']\n",
      "pred = ['an', 'older', 'couple', 'car', 'running', 'through', 'the', 'grass', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'junge', 'in', 'shorts', 'macht', 'einen', '<unk>', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'boy', 'in', 'shorts', 'doing', 'a', 'skateboard', 'trick', '.']\n",
      "pred = ['a', 'boy', 'in', 'shorts', 'wearing', 'a', 'skateboard', 'site', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'chinese', 'sitzt', 'und', 'wartet', 'auf', 'kundschaft', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'chinese', 'man', 'sitting', 'down', 'waiting', 'for', 'customers', '.']\n",
      "pred = ['a', 'shirtless', 'man', 'sitting', 'at', 'something', 'for', 'night', '.']\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "for example_idx in range(10) : \n",
    "    for i in test_iterator : \n",
    "        break\n",
    "\n",
    "    sent = []\n",
    "    for wi in i.src[example_idx][i.src[example_idx] != 1] : \n",
    "        wi = wi.cpu().data.numpy().item()\n",
    "        txt = SRC.vocab.itos[wi]\n",
    "        sent.append(txt)\n",
    "    print(f'src = {sent}')\n",
    "\n",
    "    for i in test_iterator : \n",
    "        break\n",
    "\n",
    "    sent = []\n",
    "    for wi in i.trg[example_idx][i.src[example_idx] != 1] : \n",
    "        wi = wi.cpu().data.numpy().item()\n",
    "        txt = TRG.vocab.itos[wi]\n",
    "        sent.append(txt)\n",
    "    print(f'trg = {sent}')\n",
    "\n",
    "    model.eval()\n",
    "    output = model(i.src, i.trg[:,:-1])\n",
    "    predictions = output[example_idx].argmax(1)\n",
    "\n",
    "    sent = []\n",
    "    for wi in predictions : \n",
    "        wi = wi.cpu().data.numpy().item()\n",
    "        if wi == TRG.vocab.stoi['<eos>'] : \n",
    "            break\n",
    "        txt = TRG.vocab.itos[wi]\n",
    "        sent.append(txt)\n",
    "    print(f'pred = {sent}')\n",
    "\n",
    "    print(\"#\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
