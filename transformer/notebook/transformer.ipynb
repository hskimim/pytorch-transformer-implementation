{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:36.014884Z",
     "start_time": "2022-01-23T17:15:31.559215Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:36.025797Z",
     "start_time": "2022-01-23T17:15:36.017827Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "- copied from https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:40.399227Z",
     "start_time": "2022-01-23T17:15:36.028390Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:40.406618Z",
     "start_time": "2022-01-23T17:15:40.401833Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.831220Z",
     "start_time": "2022-01-23T17:15:40.409282Z"
    }
   },
   "outputs": [],
   "source": [
    "src_seq_len = 30\n",
    "trg_seq_len = 30-1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            fix_length=src_seq_len,\n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            fix_length=src_seq_len,\n",
    "            batch_first = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)\n",
    "\n",
    "for i in train_iterator : \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.854232Z",
     "start_time": "2022-01-23T17:15:49.836432Z"
    }
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d_model)\n",
    "    \n",
    "    def generate_enc_mask_m(self, src) :       \n",
    "        mask_m = (src != 1).unsqueeze(1).unsqueeze(2)\n",
    "        return mask_m\n",
    "\n",
    "    def generate_dec_mask_m(self, src, trg) :\n",
    "        trg_pad_mask = (trg != 1).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len)), diagonal=0).bool().to(trg_pad_mask.device)\n",
    "        mask_m = trg_pad_mask & trg_sub_mask\n",
    "        return mask_m\n",
    "    \n",
    "    def forward(self, x) : \n",
    "        emb = self.tok_emb(x)\n",
    "        pos = torch.arange(0, emb.shape[1]).unsqueeze(0).repeat(emb.shape[0], 1).to(emb.device)\n",
    "        summed = emb / math.sqrt(self.d_model) + self.pos_emb(pos)\n",
    "        return summed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.869520Z",
     "start_time": "2022-01-23T17:15:49.856647Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module) : \n",
    "    def __init__(self, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask) :         \n",
    "        score = torch.matmul(q, k.permute(0,1,3,2).contiguous())/math.sqrt(d_model)\n",
    "        score = score.masked_fill(mask, -1e10)\n",
    "        scaled_score = torch.softmax(score, dim=-1)\n",
    "        \n",
    "        attention = torch.matmul(scaled_score, v).permute(0,2,3,1).contiguous()\n",
    "        attention = attention.view(attention.shape[0], attention.shape[1], self.d_model)\n",
    "        \n",
    "        return self.fc(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.886567Z",
     "start_time": "2022-01-23T17:15:49.872469Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) : \n",
    "    def __init__(self, d_model, seq_length, n_head) : \n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, f\"n_head({n_head}) does not divide d_model({d_model})\"\n",
    "\n",
    "        self.n_div_head = d_model//n_head\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_length\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.Q = nn.Linear(d_model,  d_model)\n",
    "        self.K = nn.Linear(d_model,  d_model)\n",
    "        self.V = nn.Linear(d_model,  d_model)\n",
    "        \n",
    "    def div_and_sort_for_multiheads(self, projected, seq_len) : \n",
    "        div = projected.view(projected.shape[0], self.n_head, seq_len, self.n_div_head)\n",
    "        return div\n",
    "    \n",
    "    def forward(self, emb, enc_inputs=None) :\n",
    "        q = self.div_and_sort_for_multiheads(self.Q(emb), self.seq_len)\n",
    "    \n",
    "        if enc_inputs is not None : # enc-dec attention\n",
    "            seq_len = enc_inputs.shape[1] # takes target sequence length for k and v\n",
    "            k = self.div_and_sort_for_multiheads(self.K(enc_inputs), seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(enc_inputs), seq_len)\n",
    "        else : # self-attention\n",
    "            k = self.div_and_sort_for_multiheads(self.K(emb), self.seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(emb), self.seq_len)\n",
    "\n",
    "        return q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process the sub-layer\n",
    "- layer normalization\n",
    "- residual conection\n",
    "- residual dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.897601Z",
     "start_time": "2022-01-23T17:15:49.891486Z"
    }
   },
   "outputs": [],
   "source": [
    "class PostProcessing(nn.Module) : \n",
    "    def __init__(self, d_model, p=0.1) : \n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, emb, attn) : \n",
    "        return self.ln(emb+self.dropout(attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.906529Z",
     "start_time": "2022-01-23T17:15:49.900648Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module) : \n",
    "    def __init__(self, d_model, d_ff) : \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        return self.fc2(torch.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.917185Z",
     "start_time": "2022-01-23T17:15:49.908976Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model, d_ff, n_head, dropout_p) : \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ma = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.sdp = ScaledDotProductAttention(d_model)\n",
    "        \n",
    "        self.pp1 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp2 = PostProcessing(d_model, dropout_p)\n",
    "        \n",
    "        self.positionwise_ffn = PositionwiseFFN(d_model, d_ff)\n",
    "            \n",
    "    def forward(self, emb, mask_m) :\n",
    "\n",
    "        q,k,v = self.ma(emb)    \n",
    "        attn = self.sdp(q,k,v, mask=mask_m)\n",
    "        \n",
    "        attn = self.pp1(emb, attn)\n",
    "        z = self.positionwise_ffn(attn)\n",
    "\n",
    "        return self.pp2(attn, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.929653Z",
     "start_time": "2022-01-23T17:15:49.919058Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model, d_ff, n_head, dropout_p) : \n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.ma_self = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.ma_enc_dec = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.sdp_self = ScaledDotProductAttention(d_model)\n",
    "        self.sdp_enc_dec = ScaledDotProductAttention(d_model)\n",
    "        \n",
    "        self.pp1 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp2 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp3 = PostProcessing(d_model, dropout_p)\n",
    "\n",
    "        self.positionwise_ffn = PositionwiseFFN(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, emb, mask_m_src, mask_m_trg, enc_hidden) : \n",
    "        \n",
    "        q,k,v = self.ma_self(emb)\n",
    "        attn = self.sdp_self(q,k,v, mask=mask_m_trg)\n",
    "        attn1 = self.pp1(emb, attn)\n",
    "        \n",
    "        dec_q,enc_k,enc_v = self.ma_enc_dec(attn1, enc_hidden)\n",
    "        attn2 = self.sdp_enc_dec(dec_q, enc_k, enc_v, mask_m_src)\n",
    "        sub_layer_output = self.pp2(attn1, attn2)\n",
    "\n",
    "        z = self.positionwise_ffn(sub_layer_output)\n",
    "\n",
    "        return self.pp3(sub_layer_output, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.956050Z",
     "start_time": "2022-01-23T17:15:49.933378Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module) : \n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 trg_vocab_size,\n",
    "                 src_seq_length,\n",
    "                 trg_seq_length,\n",
    "                 d_model,\n",
    "                 d_ff,\n",
    "                 n_head,\n",
    "                 dropout_p,\n",
    "                 n_enc_layer,\n",
    "                 n_dec_layer) : \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.src_embber = InputEmbedding(src_vocab_size, src_seq_length, d_model)\n",
    "        self.trg_embber = InputEmbedding(trg_vocab_size, trg_seq_length, d_model)\n",
    "        \n",
    "        enc = EncoderLayer(src_vocab_size, src_seq_length, d_model, d_ff, n_head, dropout_p)\n",
    "        dec = DecoderLayer(trg_vocab_size, trg_seq_length, d_model, d_ff, n_head, dropout_p)\n",
    "        \n",
    "        self.enc = nn.ModuleList([deepcopy(enc) for _ in range(n_enc_layer)])\n",
    "        self.dec = nn.ModuleList([deepcopy(dec) for _ in range(n_dec_layer)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, src, trg) : \n",
    "        \n",
    "        src_emb, trg_emb = self.src_embber(src), self.trg_embber(trg)\n",
    "        src_mask_m = self.src_embber.generate_enc_mask_m(src)\n",
    "        trg_mask_m = self.trg_embber.generate_enc_mask_m(trg)\n",
    "        \n",
    "        for enc_layer in self.enc : \n",
    "            src_emb = enc_layer(src_emb, src_mask_m)\n",
    "        \n",
    "        for dec_layer in self.dec : \n",
    "            trg_emb = dec_layer(trg_emb, src_mask_m, trg_mask_m, src_emb)\n",
    "        \n",
    "        return self.fc(trg_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:49.967033Z",
     "start_time": "2022-01-23T17:15:49.959477Z"
    }
   },
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "d_ff = 512\n",
    "n_head = 8\n",
    "batch_size = BATCH_SIZE\n",
    "src_vocab_size = len(SRC.vocab)\n",
    "trg_vocab_size = len(TRG.vocab)\n",
    "dropout_p = 0.1\n",
    "n_enc_layer, n_dec_layer = 3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:50.284472Z",
     "start_time": "2022-01-23T17:15:49.970946Z"
    }
   },
   "outputs": [],
   "source": [
    "model = EncoderDecoder(src_vocab_size,\n",
    "                         trg_vocab_size,\n",
    "                         src_seq_len,\n",
    "                         trg_seq_len,\n",
    "                         d_model,\n",
    "                         d_ff,\n",
    "                         n_head,\n",
    "                         dropout_p,\n",
    "                         n_enc_layer,\n",
    "                         n_dec_layer).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:15:50.302534Z",
     "start_time": "2022-01-23T17:15:50.298045Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator) :\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:,:-1])                \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg[:,:-1])            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:22:17.604375Z",
     "start_time": "2022-01-23T17:16:56.853478Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 31s\n",
      "\tTrain Loss: 4.778 | Train PPL: 118.877\n",
      "\t Val. Loss: 3.802 |  Val. PPL:  44.784\n",
      "Epoch: 02 | Time: 0m 32s\n",
      "\tTrain Loss: 3.466 | Train PPL:  32.002\n",
      "\t Val. Loss: 2.934 |  Val. PPL:  18.795\n",
      "Epoch: 03 | Time: 0m 32s\n",
      "\tTrain Loss: 2.852 | Train PPL:  17.321\n",
      "\t Val. Loss: 2.411 |  Val. PPL:  11.147\n",
      "Epoch: 04 | Time: 0m 32s\n",
      "\tTrain Loss: 2.415 | Train PPL:  11.193\n",
      "\t Val. Loss: 2.006 |  Val. PPL:   7.433\n",
      "Epoch: 05 | Time: 0m 31s\n",
      "\tTrain Loss: 2.035 | Train PPL:   7.649\n",
      "\t Val. Loss: 1.657 |  Val. PPL:   5.244\n",
      "Epoch: 06 | Time: 0m 31s\n",
      "\tTrain Loss: 1.707 | Train PPL:   5.515\n",
      "\t Val. Loss: 1.384 |  Val. PPL:   3.991\n",
      "Epoch: 07 | Time: 0m 31s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.304\n",
      "\t Val. Loss: 1.214 |  Val. PPL:   3.368\n",
      "Epoch: 08 | Time: 0m 32s\n",
      "\tTrain Loss: 1.272 | Train PPL:   3.569\n",
      "\t Val. Loss: 1.073 |  Val. PPL:   2.924\n",
      "Epoch: 09 | Time: 0m 32s\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.065\n",
      "\t Val. Loss: 0.935 |  Val. PPL:   2.547\n",
      "Epoch: 10 | Time: 0m 31s\n",
      "\tTrain Loss: 0.989 | Train PPL:   2.688\n",
      "\t Val. Loss: 0.853 |  Val. PPL:   2.346\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:24:18.131030Z",
     "start_time": "2022-01-23T17:24:17.825419Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.867 | Test PPL:   2.379 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T17:45:48.563994Z",
     "start_time": "2022-01-23T17:45:48.015770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['<sos>', 'zwei', 'mittelgroße', 'hunde', 'laufen', 'über', 'den', 'schnee', '.', '<eos>']\n",
      "trg = ['<sos>', 'two', 'medium', 'sized', 'dogs', 'run', 'across', 'the', 'snow', '.']\n",
      "pred = ['two', 'rugby', 'some', 'dogs', 'run', 'across', 'the', 'snow', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'vier', 'personen', 'spielen', 'fußball', 'auf', 'einem', 'strand', '.', '<eos>']\n",
      "trg = ['<sos>', 'four', 'people', 'are', 'playing', 'soccer', 'on', 'a', 'beach', '.']\n",
      "pred = ['four', 'people', 'are', 'playing', 'soccer', 'on', 'a', 'beach', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'junge', 'fährt', 'skateboard', 'auf', 'einer', 'skateboardrampe', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'boy', 'riding', 'a', 'skateboard', 'on', 'a', 'skateboarding', 'ramp']\n",
      "pred = ['a', 'boy', 'riding', 'a', 'skateboard', 'on', 'a', 'rope', 'ramp']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'hund', 'springt', 'durch', 'ein', 'brennendes', 'hindernis', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'dog', 'is', 'jumping', 'through', 'a', '<unk>', 'obstacle', '.']\n",
      "pred = ['a', 'dog', 'is', 'jumping', 'through', 'a', '<unk>', 'waters', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'die', 'zwei', 'kinder', 'spielen', 'auf', 'dem', 'spielplatz', '.', '<eos>']\n",
      "trg = ['<sos>', 'the', 'two', 'kids', 'are', 'playing', 'at', 'the', 'playground', '.']\n",
      "pred = ['the', 'two', 'kids', 'are', 'playing', 'at', 'the', 'grass', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'mädchen', 'in', 'einem', '<unk>', 'mit', 'offenem', 'mund', '<eos>']\n",
      "trg = ['<sos>', 'girl', 'wearing', 'radio', 't', '-', 'shirt', 'has', 'open', 'mouth']\n",
      "pred = ['girl', 'wearing', 'protective', 'or', '-', 'shirt', 'has', 'their', 'mouth']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'brauner', 'hund', 'läuft', 'über', 'den', 'sandstrand', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'brown', 'dog', 'runs', 'down', 'the', 'sandy', 'beach', '.']\n",
      "pred = ['a', 'brown', 'dog', 'runs', 'down', 'the', 'snowy', 'beach', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'asiatischer', 'fabrikarbeiter', 'posiert', 'für', 'die', 'kamera', '.', '<eos>']\n",
      "trg = ['<sos>', 'an', 'asian', 'factory', 'worker', 'posing', 'for', 'the', 'camera', '.']\n",
      "pred = ['an', 'asian', 'ocean', 'worker', 'posing', 'for', 'the', 'camera', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'junge', 'in', 'shorts', 'macht', 'einen', '<unk>', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'boy', 'in', 'shorts', 'doing', 'a', 'skateboard', 'trick', '.']\n",
      "pred = ['a', 'boy', 'in', 'shorts', 'doing', 'a', 'skateboard', 'trick', '.']\n",
      "####################################################################################################\n",
      "src = ['<sos>', 'ein', 'chinese', 'sitzt', 'und', 'wartet', 'auf', 'kundschaft', '.', '<eos>']\n",
      "trg = ['<sos>', 'a', 'chinese', 'man', 'sitting', 'down', 'waiting', 'for', 'customers', '.']\n",
      "pred = ['a', 'lone', 'man', 'sitting', 'down', 'work', 'for', 'customers', '.']\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "for example_idx in range(10) : \n",
    "    for i in test_iterator : \n",
    "        break\n",
    "\n",
    "    sent = []\n",
    "    for wi in i.src[example_idx][i.src[example_idx] != 1] : \n",
    "        wi = wi.cpu().data.numpy().item()\n",
    "        txt = SRC.vocab.itos[wi]\n",
    "        sent.append(txt)\n",
    "    print(f'src = {sent}')\n",
    "\n",
    "    for i in test_iterator : \n",
    "        break\n",
    "\n",
    "    sent = []\n",
    "    for wi in i.trg[example_idx][i.src[example_idx] != 1] : \n",
    "        wi = wi.cpu().data.numpy().item()\n",
    "        txt = TRG.vocab.itos[wi]\n",
    "        sent.append(txt)\n",
    "    print(f'trg = {sent}')\n",
    "\n",
    "    model.eval()\n",
    "    output = model(i.src, i.trg[:,:-1])\n",
    "    predictions = output[example_idx].argmax(1)\n",
    "\n",
    "    sent = []\n",
    "    for wi in predictions : \n",
    "        wi = wi.cpu().data.numpy().item()\n",
    "        if wi == TRG.vocab.stoi['<eos>'] : \n",
    "            break\n",
    "        txt = TRG.vocab.itos[wi]\n",
    "        sent.append(txt)\n",
    "    print(f'pred = {sent}')\n",
    "\n",
    "    print(\"#\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
