# Pytorch-transformer-implementation

Several models of transformer series are developed with pytorch framework. 

Model implementation continues!

TODO : 

    - Write training code for the same model using huggingface library.
    - Work on modules for evaluation metrics such as GLUE and Squard, learn about the metric for each model and develop evaluation
    - When the path of raw text is inserted, it is converted into a format for model training (ex. mlm) and works a pipeline that even trains.
    - Set-up the reproduced experimental example code in the colab environment and share the link
____________

### Deployed    
1. [Transformer](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/transformer)
    - [GLU variants](https://arxiv.org/pdf/2002.05202.pdf) are supported in point-wise feed forward network
        
2. [GPT1](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/gpt1)

3. [BERT](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/bert)

4. [ALBERT](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/albert)

5. [Set-Transformer](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/set_transformer)

6. [Linformer](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/linformer)

7. [Synthesizer](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/synthesizer)

8. [Vision-Transformer](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/vision_transformer)

### Working

The list below is for those being implemented or being debugged.

1. [Linear-transformer](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/linear_transformer)

2. [Sparse-Transformer](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/sparse_transformer)

3. [DeiT](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/deit)

4. [MAE](https://github.com/hskimim/pytorch-transformer-implementation/tree/master/mae)

### TODO 

Below is a list of papers that are being read.

- [Transformer Quality in Linear Time](https://arxiv.org/pdf/2202.10447.pdf)

- [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/pdf/2202.08791)

- [Rethinking Attention with Performers](https://arxiv.org/pdf/2009.14794)
