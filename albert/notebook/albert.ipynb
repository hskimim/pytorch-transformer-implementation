{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.687515Z",
     "start_time": "2022-02-02T14:09:43.431489Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.693924Z",
     "start_time": "2022-02-02T14:09:44.689703Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.714418Z",
     "start_time": "2022-02-02T14:09:44.698536Z"
    }
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model//6, padding_idx=vocab.pad_id)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d_model//6)\n",
    "        self.seg_emb = nn.Embedding(3, d_model//6, padding_idx=vocab.pad_id)\n",
    "\n",
    "        # embedding matrix parameterization\n",
    "        self.tok_proj = nn.Linear(d_model//6, d_model)\n",
    "        self.pos_proj = nn.Linear(d_model//6, d_model)\n",
    "        self.seg_proj = nn.Linear(d_model//6, d_model)\n",
    "        \n",
    "        self.dp = nn.Dropout(0.1)\n",
    "        \n",
    "    def generate_enc_mask_m(self, src) :       \n",
    "        mask_m = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        return mask_m\n",
    "    \n",
    "    def forward(self, txt, seg) : \n",
    "        emb = self.tok_emb(txt)\n",
    "        pos = torch.arange(0, emb.shape[1]).unsqueeze(0).repeat(emb.shape[0], 1).to(emb.device)\n",
    "        summed = self.tok_proj(emb) + self.pos_proj(self.pos_emb(pos)) + self.seg_proj(self.seg_emb(seg))\n",
    "        return self.dp(summed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.725933Z",
     "start_time": "2022-02-02T14:09:44.717891Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module) : \n",
    "    def __init__(self, d_model) : \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask) :         \n",
    "        score = torch.matmul(q, k.permute(0,1,3,2).contiguous())/math.sqrt(d_model)\n",
    "        score = score.masked_fill(mask, -1e10)\n",
    "        scaled_score = torch.softmax(score, dim=-1)\n",
    "        \n",
    "        attention = torch.matmul(scaled_score, v).permute(0,2,3,1).contiguous()\n",
    "        attention = attention.view(attention.shape[0], attention.shape[1], self.d_model)\n",
    "        \n",
    "        return self.fc(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.737955Z",
     "start_time": "2022-02-02T14:09:44.727707Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) : \n",
    "    def __init__(self, d_model, seq_length, n_head) : \n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, f\"n_head({n_head}) does not divide d_model({d_model})\"\n",
    "\n",
    "        self.n_div_head = d_model//n_head\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_length\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.Q = nn.Linear(d_model,  d_model)\n",
    "        self.K = nn.Linear(d_model,  d_model)\n",
    "        self.V = nn.Linear(d_model,  d_model)\n",
    "        \n",
    "    def div_and_sort_for_multiheads(self, projected, seq_len) : \n",
    "        div = projected.view(projected.shape[0], self.n_head, seq_len, self.n_div_head)\n",
    "        return div\n",
    "    \n",
    "    def forward(self, emb, enc_inputs=None) :\n",
    "        q = self.div_and_sort_for_multiheads(self.Q(emb), self.seq_len)\n",
    "    \n",
    "        if enc_inputs is not None : # enc-dec attention\n",
    "            seq_len = enc_inputs.shape[1] # takes target sequence length for k and v\n",
    "            k = self.div_and_sort_for_multiheads(self.K(enc_inputs), seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(enc_inputs), seq_len)\n",
    "        else : # self-attention\n",
    "            k = self.div_and_sort_for_multiheads(self.K(emb), self.seq_len)\n",
    "            v = self.div_and_sort_for_multiheads(self.V(emb), self.seq_len)\n",
    "\n",
    "        return q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process the sub-layer\n",
    "- layer normalization\n",
    "- residual conection\n",
    "- residual dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.747548Z",
     "start_time": "2022-02-02T14:09:44.740131Z"
    }
   },
   "outputs": [],
   "source": [
    "class PostProcessing(nn.Module) : \n",
    "    def __init__(self, d_model, p=0.1) : \n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, emb, attn) : \n",
    "        return emb+self.dropout(self.ln(attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.754492Z",
     "start_time": "2022-02-02T14:09:44.749843Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module) : \n",
    "    def __init__(self, d_model, d_ff) : \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        return self.fc2(torch.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.764672Z",
     "start_time": "2022-02-02T14:09:44.757583Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module) : \n",
    "    def __init__(self, vocab_size, seq_length, d_model, d_ff, n_head, dropout_p) : \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ma = MultiHeadAttention(d_model, seq_length, n_head).to(device)\n",
    "        self.sdp = ScaledDotProductAttention(d_model)\n",
    "        \n",
    "        self.pp1 = PostProcessing(d_model, dropout_p)\n",
    "        self.pp2 = PostProcessing(d_model, dropout_p)\n",
    "        \n",
    "        self.positionwise_ffn = PositionwiseFFN(d_model, d_ff)\n",
    "            \n",
    "    def forward(self, emb, mask_m) :\n",
    "\n",
    "        q,k,v = self.ma(emb)    \n",
    "        attn = self.sdp(q,k,v, mask=mask_m)\n",
    "        \n",
    "        attn = self.pp1(emb, attn)\n",
    "        z = self.positionwise_ffn(attn)\n",
    "\n",
    "        return self.pp2(attn, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.773261Z",
     "start_time": "2022-02-02T14:09:44.766461Z"
    }
   },
   "outputs": [],
   "source": [
    "class ALBERT(nn.Module) : \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 seq_length,\n",
    "                 d_model,\n",
    "                 d_ff,\n",
    "                 n_head,\n",
    "                 dropout_p,\n",
    "                 n_enc_layer) : \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embber = InputEmbedding(vocab_size, seq_length, d_model)\n",
    "        \n",
    "        enc = EncoderLayer(vocab_size, seq_length, d_model, d_ff, n_head, dropout_p)\n",
    "        \n",
    "        self.enc = nn.ModuleList([enc for _ in range(n_enc_layer)]) # parameter-sharing\n",
    "\n",
    "    def forward(self, txt, seg) : \n",
    "        \n",
    "        emb = self.embber(txt, seg)\n",
    "        mask_m = self.embber.generate_enc_mask_m(txt)\n",
    "\n",
    "        for enc_layer in self.enc : \n",
    "            emb = enc_layer(emb, mask_m)\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:44.780302Z",
     "start_time": "2022-02-02T14:09:44.775072Z"
    }
   },
   "outputs": [],
   "source": [
    "class AlbertFC(nn.Module) : \n",
    "    def __init__(self, embedder, d_model, vocab_size) : \n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.mlm_fc = nn.Linear(d_model, vocab_size)\n",
    "        self.nsp_fc = nn.Linear(d_model, 2)\n",
    "    \n",
    "    def forward(self, txt, seg) : \n",
    "        emb = self.embedder(txt, seg)\n",
    "        return torch.log_softmax(self.mlm_fc(emb), dim=-1), torch.log_softmax(self.nsp_fc(emb[:,0]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:45.071582Z",
     "start_time": "2022-02-02T14:09:44.782074Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "seq_len = 256\n",
    "\n",
    "train_dataset = iterator.ALBertIterator(filename = '../../bert/data/wikitext-2-raw/prep_train.txt',\n",
    "                                tokenizer_model_path = '../../bert/data/wiki_pretrained_vocab/m.model',\n",
    "                                seq_len=seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    " \n",
    "valid_dataset = iterator.ALBertIterator(filename = '../../bert/data/wikitext-2-raw/prep_valid.txt',\n",
    "                                tokenizer_model_path = '../../bert/data/wiki_pretrained_vocab/m.model',\n",
    "                                seq_len=seq_len)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:45.155963Z",
     "start_time": "2022-02-02T14:09:45.074336Z"
    }
   },
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "d_ff = d_model * 4\n",
    "n_head = d_model // 64\n",
    "vocab_size = 30000\n",
    "dropout_p = 0.1\n",
    "n_enc_layer = 5\n",
    "seq_length = train_dataset.seq_len\n",
    "vocab = train_dataset.vocab\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:49.688835Z",
     "start_time": "2022-02-02T14:09:45.159573Z"
    }
   },
   "outputs": [],
   "source": [
    "albert_ember = ALBERT(vocab_size,\n",
    "             seq_length,\n",
    "             d_model,\n",
    "             d_ff,\n",
    "             n_head,\n",
    "             dropout_p,\n",
    "             n_enc_layer).to(device) \n",
    "# generate embeding using transformer architecture\n",
    "\n",
    "albert_predicter = nn.DataParallel(AlbertFC(albert_ember, d_model, vocab_size)).to(device)\n",
    "# return 2 fc layer for training bi-directional representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:49.701068Z",
     "start_time": "2022-02-02T14:09:49.691387Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9804176"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for p in albert_predicter.named_parameters() : \n",
    "    num_params = p[1].nelement()\n",
    "    total_parameters += num_params\n",
    "total_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:49.713902Z",
     "start_time": "2022-02-02T14:09:49.703194Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True) # -- for debugging the train progress --\n",
    "N_EPOCHS = 20\n",
    "\n",
    "criterion1 = nn.NLLLoss(ignore_index=0).to(device)\n",
    "criterion2 = nn.NLLLoss().to(device)\n",
    "\n",
    "optimizer = optim.AdamW(albert_predicter.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-1, pct_start=0.01, \n",
    "                                          steps_per_epoch=len(train_dataloader), epochs=N_EPOCHS, \n",
    "                                          total_steps=N_EPOCHS * len(train_dataloader), anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T14:09:49.730099Z",
     "start_time": "2022-02-02T14:09:49.715627Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, scheduler):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_total_loss = 0\n",
    "    epoch_mlm_loss = 0\n",
    "    epoch_nsp_loss = 0 \n",
    "    \n",
    "    epoch_mlm_acc = 0    \n",
    "    epoch_nsp_acc = 0    \n",
    "    cnt = 0\n",
    "    \n",
    "    for data in tqdm(dataloader, desc='train') :\n",
    "        \n",
    "        data = {k:v.to(device) for k,v in data.items()}\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        mlm_pred, nsp_pred = model(data['text'], data['seg'])\n",
    "\n",
    "        # calculate loss from masked language modeling\n",
    "        mlm_loss = criterion1(mlm_pred.transpose(1,2), data['mlm'])\n",
    "        \n",
    "        # calculate loss from next sentence prediction\n",
    "        nsp_loss = criterion2(nsp_pred, data['nsp'].long().cuda())\n",
    "        \n",
    "        # merge two loss equally\n",
    "        loss = mlm_loss + nsp_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()        \n",
    "        \n",
    "        # calculate acc for mlm\n",
    "        acc = (mlm_pred.view(-1,vocab_size).argmax(1) == data['mlm'].view(-1)).sum().item() / data['mlm'].view(-1).shape[0]\n",
    "        epoch_mlm_acc += acc\n",
    "        \n",
    "        # calculate acc from nsp\n",
    "        acc = (nsp_pred.argmax(1) == data['nsp']).sum() / data['nsp'].shape[0]\n",
    "        epoch_nsp_acc += acc\n",
    "        \n",
    "        epoch_total_loss += loss.item()\n",
    "        epoch_mlm_loss += mlm_loss.item()\n",
    "        epoch_nsp_loss += nsp_loss.item()        \n",
    "        cnt += 1\n",
    "        scheduler.step()\n",
    "        \n",
    "    print(f'\\tTrain Total Loss: {epoch_total_loss / cnt:.3f} | Train MLM Loss: {epoch_mlm_loss / cnt:.3f} | Train NSP Loss: {epoch_nsp_loss / cnt:.3f}\\\n",
    "        | MLM ACC : {epoch_mlm_acc / cnt: .3f} | NSP ACC : {epoch_nsp_acc / cnt: .3f} | Learning Rate : {scheduler.get_last_lr()[0]:.3f}')\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_total_loss = 0\n",
    "    epoch_mlm_loss = 0\n",
    "    epoch_nsp_loss = 0 \n",
    "    \n",
    "    epoch_mlm_acc = 0    \n",
    "    epoch_nsp_acc = 0    \n",
    "    cnt = 0\n",
    "    \n",
    "    with torch.no_grad() : \n",
    "        for data in tqdm(dataloader, desc='valid') :\n",
    "            \n",
    "            data = {k:v.to(device) for k,v in data.items()}\n",
    "\n",
    "            mlm_pred, nsp_pred = model(data['text'], data['seg'])\n",
    "            \n",
    "            mlm_loss = criterion1(mlm_pred.transpose(1,2), data['mlm'])\n",
    "\n",
    "            nsp_loss = criterion2(nsp_pred, data['nsp'].long().cuda())\n",
    "            \n",
    "            loss = mlm_loss + nsp_loss\n",
    "            \n",
    "            acc = (mlm_pred.view(-1,vocab_size).argmax(1) == data['mlm'].view(-1)).sum().item() / data['mlm'].view(-1).shape[0]\n",
    "            epoch_mlm_acc += acc\n",
    "\n",
    "            acc = (nsp_pred.argmax(1) == data['nsp']).sum() / data['nsp'].shape[0]\n",
    "            epoch_nsp_acc += acc\n",
    "\n",
    "            epoch_total_loss += loss.item()\n",
    "            epoch_mlm_loss += mlm_loss.item()\n",
    "            epoch_nsp_loss += nsp_loss.item()        \n",
    "            cnt += 1\n",
    "\n",
    "        print(f'\\tValid Total Loss: {epoch_total_loss / cnt:.3f} | Valid MLM Loss: {epoch_mlm_loss / cnt:.3f} | Valid NSP Loss: {epoch_nsp_loss / cnt:.3f}\\\n",
    "            | MLM ACC : {epoch_mlm_acc / cnt: .3f} | NSP ACC : {epoch_nsp_acc / cnt: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T15:29:50.482125Z",
     "start_time": "2022-02-02T14:09:49.731634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################################################################################################\n",
      "Epoch : 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4dab049281481e851884b3c1c2168b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 58.416 | Train MLM Loss: 55.266 | Train NSP Loss: 3.150        | MLM ACC :  0.002 | NSP ACC :  0.508 | Learning Rate : 0.096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea94fbe5e9454debb4fd3d759acd5758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 90.351 | Valid MLM Loss: 82.862 | Valid NSP Loss: 7.488            | MLM ACC :  0.002 | NSP ACC :  0.502\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2df5a78d09c4e38a396d34e23721c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 65.501 | Train MLM Loss: 63.456 | Train NSP Loss: 2.045        | MLM ACC :  0.003 | NSP ACC :  0.515 | Learning Rate : 0.091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a6caf96d5f4d569d73da3ef5d54ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 57.486 | Valid MLM Loss: 56.738 | Valid NSP Loss: 0.748            | MLM ACC :  0.005 | NSP ACC :  0.514\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dec6bdc57942dc84ab91da912e6134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 59.866 | Train MLM Loss: 57.752 | Train NSP Loss: 2.114        | MLM ACC :  0.003 | NSP ACC :  0.497 | Learning Rate : 0.086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42e21fb98aa4547bbffa6deec9e9060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 45.804 | Valid MLM Loss: 44.837 | Valid NSP Loss: 0.966            | MLM ACC :  0.001 | NSP ACC :  0.499\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c73fab538c8464b88b90b9a27044f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 36.997 | Train MLM Loss: 35.824 | Train NSP Loss: 1.173        | MLM ACC :  0.002 | NSP ACC :  0.503 | Learning Rate : 0.081\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452c543db5ad474cbefa6cdac8de6b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 33.179 | Valid MLM Loss: 31.857 | Valid NSP Loss: 1.322            | MLM ACC :  0.004 | NSP ACC :  0.505\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5c860f5b524fe18b18c80d0a8a7c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 31.101 | Train MLM Loss: 30.038 | Train NSP Loss: 1.063        | MLM ACC :  0.003 | NSP ACC :  0.499 | Learning Rate : 0.076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcfe154eeea4948829554461976d2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 15.702 | Valid MLM Loss: 14.922 | Valid NSP Loss: 0.779            | MLM ACC :  0.003 | NSP ACC :  0.494\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4ad25a4616482996ffbc6476ce2166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 16.324 | Train MLM Loss: 15.321 | Train NSP Loss: 1.003        | MLM ACC :  0.004 | NSP ACC :  0.494 | Learning Rate : 0.071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4dc8b7c5eb4790a88e36d4f7444b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 8.419 | Valid MLM Loss: 7.721 | Valid NSP Loss: 0.698            | MLM ACC :  0.006 | NSP ACC :  0.511\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bcbe1ddd65443fbfcae0c38469ae7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 17.437 | Train MLM Loss: 13.481 | Train NSP Loss: 3.956        | MLM ACC :  0.004 | NSP ACC :  0.504 | Learning Rate : 0.066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb265db5ee847d8a66d9beff6964332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 8.333 | Valid MLM Loss: 7.538 | Valid NSP Loss: 0.795            | MLM ACC :  0.008 | NSP ACC :  0.501\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b9780d682f455ca0c8e0afa19aeb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 11.195 | Train MLM Loss: 10.418 | Train NSP Loss: 0.777        | MLM ACC :  0.005 | NSP ACC :  0.501 | Learning Rate : 0.061\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37278b110b34fbe94244d1bf403f0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 8.459 | Valid MLM Loss: 7.744 | Valid NSP Loss: 0.715            | MLM ACC :  0.007 | NSP ACC :  0.498\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877e0f84f565431dbf316627c6160544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 8.488 | Train MLM Loss: 7.756 | Train NSP Loss: 0.732        | MLM ACC :  0.007 | NSP ACC :  0.507 | Learning Rate : 0.056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4ad2687f1f45ac884237d77b9a42e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 8.616 | Valid MLM Loss: 7.925 | Valid NSP Loss: 0.690            | MLM ACC :  0.007 | NSP ACC :  0.544\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebff2c7bcba43cd962132e834935e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 8.326 | Train MLM Loss: 7.551 | Train NSP Loss: 0.775        | MLM ACC :  0.007 | NSP ACC :  0.494 | Learning Rate : 0.050\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c98e5536864845995bee82ceaa7b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 8.118 | Valid MLM Loss: 7.427 | Valid NSP Loss: 0.691            | MLM ACC :  0.007 | NSP ACC :  0.490\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a58761daa37411bb78fee1b616b3e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 8.595 | Train MLM Loss: 7.852 | Train NSP Loss: 0.742        | MLM ACC :  0.007 | NSP ACC :  0.511 | Learning Rate : 0.045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3c8a779bfe47ae85649533f78c66f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.811 | Valid MLM Loss: 7.102 | Valid NSP Loss: 0.709            | MLM ACC :  0.008 | NSP ACC :  0.498\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3534c21e4224694a5774c0f63e73e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 8.506 | Train MLM Loss: 7.773 | Train NSP Loss: 0.733        | MLM ACC :  0.007 | NSP ACC :  0.511 | Learning Rate : 0.040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9edf8ee406d41e29ee5cce9bff32304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.868 | Valid MLM Loss: 7.176 | Valid NSP Loss: 0.692            | MLM ACC :  0.008 | NSP ACC :  0.551\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f438e3ffdf4fbdb3ff58f57dac4cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 7.971 | Train MLM Loss: 7.244 | Train NSP Loss: 0.728        | MLM ACC :  0.008 | NSP ACC :  0.514 | Learning Rate : 0.035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1a3ada05b84258b2664ca2c815c449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.667 | Valid MLM Loss: 7.007 | Valid NSP Loss: 0.661            | MLM ACC :  0.009 | NSP ACC :  0.580\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806591e9ece842f78da3b9d8aec83514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 8.274 | Train MLM Loss: 7.535 | Train NSP Loss: 0.740        | MLM ACC :  0.008 | NSP ACC :  0.520 | Learning Rate : 0.030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff27edffb61b48d282f2079e88077851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.756 | Valid MLM Loss: 7.070 | Valid NSP Loss: 0.686            | MLM ACC :  0.008 | NSP ACC :  0.559\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dc544c402f4fb3b1ff4d0fbcf0484c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 7.991 | Train MLM Loss: 7.286 | Train NSP Loss: 0.704        | MLM ACC :  0.008 | NSP ACC :  0.541 | Learning Rate : 0.025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6d74322990416baa352d0cf3e7b7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.890 | Valid MLM Loss: 7.142 | Valid NSP Loss: 0.748            | MLM ACC :  0.008 | NSP ACC :  0.498\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d377aa93a3084cc39dcf92a9af327e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 7.836 | Train MLM Loss: 7.141 | Train NSP Loss: 0.696        | MLM ACC :  0.009 | NSP ACC :  0.542 | Learning Rate : 0.020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ada2e43e36432cb88aa4328712f111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.649 | Valid MLM Loss: 7.000 | Valid NSP Loss: 0.649            | MLM ACC :  0.009 | NSP ACC :  0.573\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bfc3bdf18844638076f68e86f6672c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 7.802 | Train MLM Loss: 7.107 | Train NSP Loss: 0.695        | MLM ACC :  0.009 | NSP ACC :  0.552 | Learning Rate : 0.015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b048f810644c9981f96bc210439e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.578 | Valid MLM Loss: 6.927 | Valid NSP Loss: 0.651            | MLM ACC :  0.010 | NSP ACC :  0.569\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe5ff4044ce4d67b0d769bbd226c389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 7.652 | Train MLM Loss: 6.983 | Train NSP Loss: 0.669        | MLM ACC :  0.009 | NSP ACC :  0.561 | Learning Rate : 0.010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23af7ed4d4d745b39abf92e048f7ac76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.554 | Valid MLM Loss: 6.911 | Valid NSP Loss: 0.643            | MLM ACC :  0.009 | NSP ACC :  0.567\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238019f617e04bcda11c1b82780d65a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 7.575 | Train MLM Loss: 6.923 | Train NSP Loss: 0.652        | MLM ACC :  0.010 | NSP ACC :  0.573 | Learning Rate : 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1074b96b704318af31596ad45125a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.491 | Valid MLM Loss: 6.861 | Valid NSP Loss: 0.630            | MLM ACC :  0.010 | NSP ACC :  0.581\n",
      "######################################################################################################################################################\n",
      "######################################################################################################################################################\n",
      "Epoch : 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2f8f6e81e242f4b77976001fadb7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Total Loss: 7.508 | Train MLM Loss: 6.875 | Train NSP Loss: 0.633        | MLM ACC :  0.010 | NSP ACC :  0.574 | Learning Rate : -0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555988d4999140e6a4d015eaf359caa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValid Total Loss: 7.450 | Valid MLM Loss: 6.829 | Valid NSP Loss: 0.621            | MLM ACC :  0.010 | NSP ACC :  0.567\n",
      "######################################################################################################################################################\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(\"###\" * 50)\n",
    "    print(f\"Epoch : {epoch+1}\")\n",
    "    train(albert_predicter, train_dataloader, optimizer, scheduler)\n",
    "    evaluate(albert_predicter, train_dataloader)\n",
    "    print(\"###\" * 50)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
